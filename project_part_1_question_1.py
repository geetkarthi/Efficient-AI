# -*- coding: utf-8 -*-
"""Project part 1 - question 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sqGOGe-HlVooY8uFVwTE0OkefR4sDIdP
"""

import math
from collections import Counter

data = [
    ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
]

data = [list(row) for row in zip(*data)]

def entropy(class_counts):
    total = sum(class_counts)
    ent = 0
    for count in class_counts:
        if count == 0:
            continue
        probability = count / total
        ent -= probability * math.log2(probability)
    return ent

def information_gain(parent_counts, subsets):
    # Parent entropy
    parent_entropy = entropy(parent_counts)

    # Weighted entropy for each subset
    total_samples = sum(parent_counts)
    weighted_entropy = 0
    for subset in subsets:
        subset_entropy = entropy(subset)
        weight = sum(subset) / total_samples
        weighted_entropy += weight * subset_entropy

    # Information gain
    gain = parent_entropy - weighted_entropy
    return gain

def split_counts(data, feature_col, target_col):
    # Dictionary to hold counts for each value in the feature column
    split_data = {}

    for row in data:
        feature_value = row[feature_col]
        target_value = row[target_col]

        # Initialize if feature_value not seen before
        if feature_value not in split_data:
            split_data[feature_value] = []
        split_data[feature_value].append(target_value)

    # Convert target values to counts
    subsets = []
    for target_values in split_data.values():
        target_counts = list(Counter(target_values).values())
        subsets.append(target_counts)

    return subsets

def calculate_information_gain(data, target_col):
    num_features = len(data[0]) - 1  # Exclude target column
    target_counts = list(Counter(row[target_col] for row in data).values())

    # Dictionary to store information gain for each feature
    gains = {}

    for feature_col in range(num_features):
        # Split data based on feature values and calculate counts
        subsets = split_counts(data, feature_col, target_col)
        # Calculate information gain
        gain = information_gain(target_counts, subsets)
        gains[f"Feature{feature_col + 1}"] = gain

    return gains

# Target column index
target_col = 4

# Calculate information gain for each feature
gains = calculate_information_gain(data, target_col)
print("Information Gain for each feature:", gains)

"""In the above results:

1. Feature 1 is Outlook
2. Feature 2 is Temperature
3. Feature 3 is Humidity
4. Feature 4 is Wind
5. The target/output variable is Response-Class.

Information gain for each feature:

1. Outlook: 0.24674981977443933
2. Temperature: 0.02922256565895487
3. Humidity: 0.15183550136234159
4. Wind: 0.04812703040826949

From the above information gain values, we can see that the nbest feature to be selected for the most accurate analysis is "Outlook" - as it has the highest information gain.
"""